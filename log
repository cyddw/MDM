6.13
1.安装FFmpeg

6.18
1.完成报错Can't pickle local object 'get_collate_fn.<locals>.<lambda>

6.19
1.完成MDM模型采样输出获取
2.完成对本文涉及的数据集的分析
3.完成对模型的损失函数分析
4.完成对模型的263个关节向量和1个特征的分析

6.20
1.为什么要将22个关节在空间的绝对位置转为263维度？(便于后续的处理，已完成)
2.完成对MDM模型输入的22*3以及263数据在模型内部的处理逻辑的分析(MDM模型输入不包括22*3，只有263维度的数据)
3.完成对MDM模型的loss的分析(计算loss时的target和生成的x_start都是263维度？是的，都是263维度。为什么采样输出为22*3？采样输出263维度，经过转化后得到22*3维度)
4.正在考虑如何将22*3维度的数据转为263维度(已完成)
5.正在探究MDM代码中是如何进行采样扩散的(已完成)
6.正在获取humanml3d的原始数据(已完成)
7.正在生成90个样本对应的vec，但是循环过程中遇到多次0-90的加载过程(已完成)

6.23
1.正在探究如何实现无条件扩散(已完成)
2.原文代码是否有误(--cond_mask_prob 0)(已完成，没问题)
3.损失函数中各类lamda的设置(已完成)
4.能否直接使用Humanml3d数据集进行无条件扩散？(先弄清楚HumanACT12输入数据的组成)
5.pose和joint3d分别代表什么？(已完成，joint3d表示关节的xyz坐标，pose表示姿态参数，即轴角表示，其中包括全局旋转表示和其他关节相对于根关节的旋转表示)
6.uncond下的cond全为True，其是如何传递至model，使其不编码text？(已完成)

6.24
1.正在复现从坐标到姿态参数的处理(已完成)
2.将mesh数据处理成可以被mdm读取的格式(已完成)

6.25
1.为什么iter=595？(已完成)
2.尝试将人体骨架mesh化(已完成)
3.为什么文件不存在？(已完成)
4.代码需要在服务器上运行(已完成)
5.下载blender(已完成)
6.如何将obj文件转为mp4(已完成)

6.26
1.获取在自己数据集下生成的mesh(已完成)
2.尝试跑通eval代码(已跑通unconditional的代码，但是需要时间太久，6h左右)
3.弄清各项评估指标的含义(已完成)
4.MDM模型中mask的目的是什么？(已完成，在训练的过程中包括有条件和无条件，以增强模型生成的多样性)

6.27
1.调试Linux服务器，并对代码环境进行测试(已完成)
2.服务器上跑代码(已完成)

6.30
1.学习TCN的相关概念(已完成)
2.学习自编码和变分自编码的相关概念(已完成)
3.完成模型整体思路构建(已完成)
4.MDM运行结果查看(已完成)
5.构建ARMD虚拟环境(已完成)

7.1
1.梳理模型整体思路以及细节(已完成)
2.尝试初步建模输入部分(已完成噪声和data的输入部分)
3.尝试修改前向传播部分(已完成)

7.2
1.逐步修改前向传播部分的代码(已完成训练部分的模型输出部分)
2.尝试修改训练部分的loss更新部分(已完成训练部分的所有内容)
3.尝试采样部分代码的修改(两个问题:1.为什么ARMD采样部分的代码是直接由X_T生成X_0的  2.pred_len、num_timesteps、seq_length、timesteps、seq_len的含义是什么)(关于问题2，具体含义是什么好像并不重要，只要知道其具体的工作原理，并将其嵌套进自己的模型中即可)
4.我的模型存在一个很致命的问题：每个样本只有最后60帧能被更新到(后续可能要改进，具体先看看模型跑出来的效果)

7.4
1.有一个疑问(在DDPM和DDIM中，都是基于加噪为高斯噪声的假设，而在ARMD中并未基于这个假设，那么论文中能用DDPM和DDIM的结论？)
2.检查并思考代码修改部分(已完成)

7.5
1.看3DGS论文
2.ARMDM模型跑了10w步还是乱码的，需要仔细查看并修改(已完成)
3.有一个疑问(因为模型是每隔60帧输出，那么输出的前60帧和后60帧是否不连贯？)
4.尝试将采样过程中的第0帧输入换成数据集的平均值，是否可行？
5.无条件采样中，输入的是(3,25,6,60)，输出是(24,3)形式还是(25,6)形式？(经过转换，输出是(24,3)形式)

7.7
1.ARMDM模型跑了20w步之后的效果仍然很差，思考能否从generate部分入手，将generate的扩散部分改为一步扩散(已完成，效果也不行，基本和输入保持一致)
2.检查ARMDM模型的训练过程是否修改有误(已完成，没有问题)
3.个人感觉是训练的loss应该设置为预测的噪声比较好(已完成，目前看来效果有改善)

7.8
1.尝试将前60帧进行复制，看看跑出来的效果(已完成)
2.下一步工作：看real-time diffusion等相关论文，看有无解决思路

7.9
1.等待代码运行结果中
2.尝试cond代码的修改(已完成)

7.10
1.查看代码中cond['y']['lengths']和cond['y']['mask']参数的作用(已完成，其中'mask'并不代表cond和uncond的比例，而是表示补齐帧和实际帧的比例)
2.看论文DiffMesh

7.14
1.弄清楚DiffMesh的模型(已完成)
2.ARMDM(0.1 trendloss substitute rotmse with previous 1 frames)：将60帧60帧的生成改成1帧1帧生成，同时给定的输入也由60帧改为1帧(1帧为训练集的前1帧)
3.已完成对loss写入csv的修改(已完成)

7.15
1.修改代码并运行(已完成)
2.尝试初步运行了一下1帧1帧输入的代码(500步)，发现最后一帧会跳帧，感觉还是loss的问题，loss应该覆盖整个GT(已完成)
3.为什么trend_mse这么小，应该是那里有问题，建议检查一下(已完成，确实有问题，需要进行一定的修改，不过在similar to DiffMesh中不用考虑trend_loss，可以注释掉)
4.generate生成的是前59帧还是后59帧(后59帧)
5.感觉给的数据集有问题，生成的GT有一半都是不动的(已将Data_loader的shuffle改为False)(已完成，数据集没问题，可能跟自己的数据集有关，采样得到的60帧不是前60帧，导致部分动画静止不动)
6.生成的GT和实际动作有出入(有较大延迟)(已完成)

7.16
1.已经找到数据集GT静止的大致原因(已完成，应该是将joints转为rot和将rot转回joints的函数有点问题，可能跟cam的位置有关系)
2.静止和延迟问题已解决(已完成)
3.增加训练步数，看看效果，同时更改loss(已完成,等待结果中)

7.18
1.进度整理(已完成)
2.尝试使用MDM模型生成采样的第一帧(已完成，效果不错)
3.进展修改(已完成)

7.21
1.弄清楚MDM的condition情形在代码中是如何实现的(已完成)
2.当前阶段任务：如何处理CSI从而得到condition
3.查找有无相关论文(已完成)

7.22
1.安装Wifi3d代码环境和依赖(已完成，代码写的太烂了，看不明白具体调用过程)
2.实现对csi的去噪操作(无法完成)

7.23
1.讨论汇总：
对模型5进行修正(将原有的60帧改为4帧，但是保证4帧的扩散步数仍然是60步)；
处理condition，首先进行去噪操作(不对幅值进行操作，只关注于csi的相位)，然后对每个receiver获取其AoA(注意，transmitter有四根天线，四根天线发送的信号是相互正交的)，最后通过各个receiver的相对位置计算得到最终的AoA；
关注diffusion flow的相关论文

7.24
1.看最小二乘线性回归算法(已完成)
2.初步完成相位去噪算法(已完成，问题1：去噪后的相位在某些子载波处发生跳变)
3.VQ-VAE学习(https://sunlin-ai.github.io/2022/06/02/VQ-VAE.html#fn:3)(看了一点)
4.贝叶斯优化学习
5.计算AoA(以UWB-Fi为参考代码)(已完成)

7.25
1.初步完成子载波中心频率划分(问题2：子载波中心频率划分不一定正确)(已完成，修改子载波中心频率的划分规则对输出结果没有影响)
2.初步完成AoA_ToF的提取(问题3：代码运算时间久，可以考虑转为torch)
3.AoA-ToF谱标签修改(已完成)

7.28
1.将代码转为torch版本，以减少运算时间(已完成，但是torch版本相比cpu版本速度提升不大)
2.获取AoA谱、AoA_Doppler谱(已完成)
3.问题汇总：(问题1：为什么aoa_doppler谱只有第一个有图像，问题2：输入csi的维度问题，是(2,1,0)，还是(2,0,1)(问题2已完成))
4.找数据集验证代码是否正确(已完成)
5.弄懂doppler等代码原理

7.29
1.弄懂val_csi的生成过程(已完成，一个直射路径和一个反射路径，直射路径路程为7m，反射路径路程长为7.56m)
2.生成以时间为x轴，AoA为y轴的图像(已完成)
3.看相关论文，思考如何将多个接收机计算的谱进行结合(已完成，初步想法：三角定位+聚类)
4.坐标转换(已完成)

7.30
1.对八个接收机的结果进行合并处理(已完成，没有丝毫效果，生成的AoA-ToF谱本身就不太对，信号源基本都集中在接收机附近，而不是物体附近。可能的原因：csi去噪过程有误；生成AoA-ToF谱的代码有误)
2.看论文video diffusion models


7.31
1.通过matlab生成无噪的csi信号(已完成)
2.看论文spotfi和wifi_localization

8.1
1.看论文wifi_localization(已完成)
2.弄明白https://zhuanlan.zhihu.com/p/611466195(看了一半，包括隐变量模型和能量函数)
3.已完成AoA_ToF代码调试，已解决之前直射路径不存在问题(信号相干，而且角度间隔小(7度左右)，导致两个信号被识别成一个信号)
4.接下来的任务：探究unwrap和去噪对结果的影响；ToF估计是否准确，考虑只用AoA定位(但是如何对信号进行pair？)

8.5
1.对unwrap的信号重新进行wrap(已完成)

8.26
1.是否unwrap对结果是否有影响？(已完成，是否unwrap对结果没有影响)

8.27
1.接下来的工作：一是看RF-diffusion和video-diffusion两篇论文；二是对生成的AoA-ToF进行静态路径的去除，重点还是对diffusion部分的改进

8.28
论文阅读进展：
1.video-diffusion(50%)
2.RF-diffusion
3.UW-wifi sensing(完成，没有过滤操作)
4.widar 2.0(完成，没有过滤操作)
5.tutorial of diffusion(50%)

8.29
1.尝试构建2D-AOA，利用两个接收端的一维AoA进行构建，效果不理想(已完成)
2.尝试用AoA-Doppler代替AoA-ToF，效果还行，且能够过滤掉静态路径分量，但是部分参数设置没弄懂(已完成)
3.已完成高通滤波和全局平均两个消除静态路径的方法，接下来需要分析使用哪个作为最终方案(使用全局平均效果相对好一点)
4.先窗口划分再滤波还是先滤波再窗口划分(先滤波再窗口划分)
5.对于2-1-1-1的例子，后半部分是静止状态，但是dfs并未集中在0hz附近，误差较大，哪里有问题？(不清楚哪里有问题，可能有其他干扰)

9.6
1.跑通mdm_between代码(已完成)
2.跑通reconstruction guidance的代码(已完成)
3.对于reconstruction guidance，修改使其给定初始第一帧(已完成)
4.对于reconstruction guidance，转为text_condition模式(已完成)
5.将自身的数据转为Humanml3d格式(已完成)
6.wifi不完整，mesh数据也不完整，先在小规模数据集上训练:
流程：1.本地data_process/Humanml3d_prc 2.本地data_process/motion_representation 3.将生成的数据转移至dataset/HumanML3D中 4.通过HumanML3D/motion_representation生成绝对值数据 5.通过HumanML3D/cal_mean_variance生成绝对值数据的均值和方差 6.通过others/cal_train_val_test生成txt文件 7.添加text文件夹至HumanML3D文件夹下
     8.添加000021.npy至dataset文件夹下
     new_joints_vecs:维度为263
     new_joints:维度为(22,3)
7.对数据集进行y轴反转(完成)
8.尝试用自身数据集对其进行训练(完成)
9.待完成的工作：1.对比学习模型的确认 2.AoA-DFS数据和Mesh数据的对齐 3.对比学习模型跑通 4.将对比学习模型整合进模型中

9.8
1.设置epoch为24000，等待训练结果(已完成，效果不错)
2.寻找对比学习的模型(主体架构还是选择Clip，其中clip的图像编码器用来对输入的AoA-Dfs进行编码，clip的文本编码器替换为其他的可以用来编码输入的human motion)
  备选方案：Tri-Modal Motion Retrieval by Learning a Joint Embedding Space(代码是java版的，且不一定完整)
3.跑通CLIP代码(已完成)
4.跑通CLIP训练代码(已完成)
5.调试CLIP训练代码，对代码整体流程进行熟悉(已完成)

9.9
1.分析CLIP代码，对其涉及文本部分的代码进行标注(已完成)
2.编写AoA-Dfs批量处理代码
3.查看MotionCLIP代码，对其motion编码器进行提取
4.对插值算法进行修改，改为python格式(已完成)

9.10
1.查找有无缺失3个点的数据(已完成)
2.将解包裹代码写入(已完成)
3.提取缺失4个包及以上的文件(已完成)
4.插值之后的结果存在拖尾(已完成，因为最大间隔不一定是800，基本都小于800)
5.代码整合+发送
6.检查代码+测试(已完成)

9.11
1.编写批量处理代码，注意对异常文件进行剔除(注意：对异常文件进行匹配和剔除、文件保存路径，确保csi和toa文件对应)(已完成)
2.测试插值前后的AoA_Dfs变化(已完成，插值前后变化不大)
3.思考如何将motion和对应的wifi信号进行对齐

9.12
1.确定组会分享的论文(已完成)

9.13
1.粗看论文diffusion forcing(已完成)
2.跑通diffusion forcing validate代码(建议:直接重新创建虚拟环境，很容易出现版本不兼容问题)(已完成)

9.14
1.跑通diffusion-forcing video和maze代码(已完成)
2.下载visio(已完成)
3.细看diffusio-forcing并总结疑问(1.什么是贝叶斯滤波，以及贝叶斯滤波在该论文中的具体用法(已完成) 2.classifier free guidance是如何推导得到的)(已完成至section3 Method)

9.15
1.细看代码sampling部分(已完成)
2.细看diffusion forcing论文的实验和附录
3.编写AoA_Dfs批量处理代码(已完成)
4.看论文diffusion forcing2(已完成)
5.openreview注册(已完成)

9.16
1.跑通diffusion forcing代码training部分(已完成)
2.细看diffusion forcing代码training部分(已完成)
3.跑通motion clip代码(已完成)
4.细看motion clip代码并重点关注motion部分的编码方法

9.17
1.再看diffusion forcing论文
问题汇总：
1.Abstract提到“ability to guide sampling to desirable trajectories”，在sampling部分是如何实现的？(已完成，通过guidance完成)
2.在训练过程中，是10帧10帧进行训练吗，这样训练不会导致生成不连贯的问题吗，还是划动着进行训练
3.论文中的ELBO是如何证明得到的，从而说明其训练的有效性(已完成)
4.论文中的Compositionality和Causal Uncertainty是什么意思
5.论文中提到的guidance是什么意思(已完成，详见论文中算法伪代码第10行)
6.Samping部分的代码部分为什么要设置Km+1和Km两个不同的行数
7.图片Full Traj.Guidance为什么未来的序列可以指导过去的序列

2.细看motion clip代码并重点关注motion部分的编码方法(调了半天，发现args应该在configure里面)(已完成)
3.修改CLIP部分的text encoder(初步完成)
4.完成motion_CLIP代码编写(先在小规模数据集(90个)上进行代码仿真，其中motion数据集地址：/home/newdisk/hch/diffusion motion inbetween/dataset/HumanML3D/new_joints_abs_3d/
                                                          AoA_Dfs数据集地址：/home/data_save_2/hch/AoA_Dfs/)
9.18
1.motion_CLIP代码问题比较多，其中主要问题是无法处理单通道图像，决定自己写代码比较好(已完成整体框架编写，接下来完成模型部分的重点搭建)

9.19
1.看DDPM理论推导部分(已完成)

9.20
1.完成motion clip代码编写(已完成)
2.完成epoch=250次的motion clip代码，问题：根本收敛不了(已完成)
3.Motion_CLIP收敛不了的可能原因：ViT-Transformer的模型过于复杂，建议换成简单的模型进行尝试(已完成，仍未收敛)
                               text_encoder权重没有更新(已完成，在实时更新)
                               motion数据需要进行标准化处理(已完成，仍然未收敛)

9.21
1.完成diffusion forcing论文的阅读和初步整理(已完成)
2.Motion_CLIP收敛不了的可能原因：数据集问题，帧和帧之间的相似度过高(间隔10个采样，仍未收敛)；输入的AoA_Dfs应该是4个，而不是单个对单个
                               模型问题，对于(128,181，120)维度的AoA_Dfs可以先过一个卷积层变为(64,181,120),然后再过一个卷积层变为(64,64,120),再过一个卷积层变为(16,64,120),再过一个卷积层(16,16,120),再展平(16*16,120)(已完成，仍未收敛)
                               AoA_Dfs数据集有问题(将motion数据集换成文本描述)(已完成，仍然未收敛)

9.22
1.Motion_CLIP收敛不了的可能原因：CLIP本身收敛慢？
2.可能找到了Motion_CLIP收敛慢的原因：数据集划分问题(一个batch_size内的数据应该尽量差异化)+AoA_Dfs本身帧和帧之间差异化不大(或者说数据本身不准确)

9.23
1.Motion_CLIP收敛问题：在Mnist数据集下，batch_size设置为100，模型没有收敛，且test结果很差；
                      在Mnist数据集下，batch_size设置为10，模型没有收敛，而且test结果全部平均，相当于模型什么都没有学到；
                      在Mnist数据集下，batch_size设置为3，模型没有收敛，而且test结果全部平均，相当于模型什么都没有学到；
                      在Mnist数据集下，batch_size设置为3，将lr由1e-3改为5e-5，模型收敛
                      在AoA_Dfs数据集下，batch_size设置为100，10，lr设置为1e-3至1e-6，模型均未收敛，而且test结果全部平均，相当于模型什么都没有学到
                      将AoA_Dfs数据集的Motion换成文本编码(Action x Position y)，batch_size设置为6，lr设置为1e-6，模型开始收敛，收敛速度较慢
                      将AoA_Dfs数据集的Motion换成文本编码(Action x Position y)，batch_size设置为6，lr设置为5e-5，模型不收敛
                      将AoA_Dfs数据集的Motion换成文本编码(Action x Position y)，batch_size设置为6，lr设置为1e-4，模型不收敛
                      将AoA_Dfs数据集的Motion换成文本编码(Action x Position y)，batch_size设置为6，lr设置为5e-6，模型收敛
                      在AoA_Dfs数据集下，batch_size设置为6，改变lr: 1e-8(不收敛)，5e-8(不收敛)，1e-7(不收敛)，5e-7(不收敛)，1e-6(不收敛)，从1e-8搜索至8e-1均不收敛，可能是Motion编码模型太简单

9.25
1.在AoA_Dfs数据集下，将Motion编码模型更换为GCN(没有收敛)
  在AoA_Dfs数据集下，将Motion编码模型更换为GCN，同时不对Motion进行归一化(没有收敛)
  在AoA_Dfs数据集下，将Motion编码模型更换为GCN，同时将Motion转为相对坐标(没有收敛)
  在AoA_Dfs数据集下，将Motion编码模型更换为Transformer，同时将Motion转为相对坐标(没有收敛)
  在AoA_Dfs数据集下，将Motion编码模型更换为text_encoder(这一方面仍然需要修改)，同时将Motion转为相对坐标(有收敛的趋势)

9.26
1.nn.init.normal_(self.joint_positional_embedding, std=0.01)，当设置位置编码初始值非常小时，在lr=1e-5下，模型不收敛，当设置std=0.1时，模型收敛
  Motion_CLIP问题汇总：1.在AoA_Dfs数据集下，对Text_encoder进行修改以适应Motion，能够收敛，但是收敛速度非常慢，且在test数据集下的准确率并不高
                      2.在AoA_Dfs数据集下，对Text_encoder进行修改以适应Motion，扩大batch_size至12，基本不怎么收敛
2.完成MDM_between部分的代码修改部分:MDM_between突然无法运行，后经排查主要原因为：在运行CLIP代码的时候使用了mdm_between的环境导致clip版本不一致
3.MDM_between部分需要修改的代码：1.更换预训练得到的模型 2.forward_backward(self, batch, cond)的cond部分增加AoA_Dfs数据 3.将MDM_Unet中的encode_text更换为AoA_Dfs编码器

9.28
1.将AoA_Dfs数据集的Motion换成文本编码(filename + "Frame" +str(i+1)),batchsize设置为12，训练120epoch，最终test结果大部分能够达到70%左右的识别率
  在AoA_Dfs数据集下，将Motion编码模型更换为text_encoder，batch_size设置为4，mask改为full-mask，选择第0个位置作为输出向量(没有收敛)
  在AoA_Dfs数据集下，将Motion编码模型更换为text_encoder，batch_size设置为4，mask改为full-mask，选择第-1个位置作为输出向量(有收敛趋势，但是速度很慢)
  在AoA_Dfs数据集下，将Motion编码模型更换为text_encoder，batch_size设置为4，mask改为full-mask，选择第-2个位置作为输出向量(没有收敛)
  (疑问)：为什么只有第-1个位置作为输出向量会收敛呢？mask不是full-mask吗？
  在AoA_Dfs数据集下，将Motion编码模型更换为text_encoder，batch_size设置为4，mask改为full-mask，选择第-1个位置作为输出向量，编码维度改为256，lr=5e-6(收敛，但是速度变慢)
  在AoA_Dfs数据集下，将Motion编码模型更换为text_encoder，batch_size设置为4，mask改为full-mask，选择第-1个位置作为输出向量，编码维度改为256，lr=1e-5(收敛，速度变快)
  在AoA_Dfs数据集下，将Motion编码模型更换为text_encoder，batch_size设置为4，mask改为full-mask，在22的关节维度上增加一行mu，其中mu增加在最前面，选择第-1个位置作为输出向量，lr=5e-6(收敛)
  在AoA_Dfs数据集下，将Motion编码模型更换为text_encoder，batch_size设置为4，mask改为full-mask，在22的关节维度上增加一行mu，其中mu增加在最前面，选择mu作为输出向量，lr=5e-6、lr=1e-5(没有收敛)
  造成上述的可能原因，包括为什么只有-1位置：CLIP的text_encoder并不是TransformerEncoder，而是ResidualAttentionBlock
  在AoA_Dfs数据集下，将Motion编码模型更换为TransformerEncoder，batch_size设置为4，mask改为full-mask，在22的关节维度上增加一行mu，其中mu增加在最前面，选择mu作为输出向量(不收敛)
  在上述基础上增加self.sigmaQuery(不收敛) 
  
2.感觉还是得对Motion编码器进行调整，尝试使用Motion_CLIP的编码器

9.29
1.在9.28.1的基础上增大了Position_Embedding的std至0.1，同时修改了Adam优化器，删除了预设的beta、eps和weight_decay，lr设置为1e-5(1e-4不收敛)，模型收敛，且速度较快(epoch 120, loss降至0.8031)
  为什么之前没有收敛：模型错误、Adam优化器设置问题(主要原因，weight_decay = 0.2, 防止过拟合，也就使得loss降不下来)
  在上述的基础上修改lr至5e-6(收敛，虽然下降速度变慢，但是能稳定下降，loss不波动)
  在上述的基础上修改Position_Embedding的std至0.01，模型loss波动很大，loss很难下降
  在上述的基础上将Position_Embedding改为Motion_CLIP的正余弦编码(能收敛，但是收敛速度不如之前的)
2.在上述的基础上将位置编码器改为Position_Embedding，同时batch_size设置为12，epoch设置为2000，lr设置为5e-6，最终loss为0.022292，test测试效果非常好，基本为对角矩阵

9.30
1.在9.29.2的基础上只对batch_size进行修改，batch_size设置为120，也就是全部，epoch设置为3000(已完成，loss降至1.1693，还未收敛，可以继续训练，当设置batch_size为12，基本为对角矩阵，当设置batch_size为120，仍有部分为0.5左右的概率)

10.1
1.在上述基础上，修改batch_size为30，epoch设置为3000
2.修改MDM_between部分的代码
疑问：MDM_between读入后的Motion经过了转换，由xyz转为了rotation形式，在对比学习时是否也要对其进行转换？
流程：1.在dataset/HumanML3D/AoADfs数据集下添加AoA_DFS数据
3.完成AoADfs和Motion_CLIP的工作汇总
